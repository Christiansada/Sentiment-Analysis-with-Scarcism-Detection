{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Based Sentiment and Sarcasm Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this jupyter notebook, we use widely available and free datasets that can be found online to train our deep neural network architecture.  The neural network architecture is multifaceted and employs different layers to serve an NLP based application.  The purpose of this architecture is being able to detect sentiment as well as detect the presence of sarcasm (sentence level).  In the notebook below we will be detailing how the architecture works by going through each of the following stages:\n",
        "\n",
        "1. Selecting and Importing Training Data\n",
        "2. Preprocessing the Training Data\n",
        "3. Creating the Deep Learning Architecture Model\n",
        "4. Training the Model\n",
        "5. Using Model for New Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Selecting and Importing Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we import all the relevant libraries that we'll possibly need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBAU2iT4R2hm"
      },
      "source": [
        "### i. Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXijL3kaf3h",
        "outputId": "129df0fa-40de-4e8c-9837-ebc69d364f02"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_eOvcx4af3o"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers \n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "\n",
        "from IPython.display import Image, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ii. Selecting the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generally speaking, the larger a dataset is for traininng purposes the more attractive it is compared to small datasets.  The reason being it gives the user more freedom to control the amount of data that the model will be trained on.  That aspect is important because if the model is trained on too much data it can cause overfitting, but if there's too little data then the model wouldn't have the chance to minimize it's loss function sufficiently.\n",
        "\n",
        "With that being said, we've chosen two datasets both of which are 5 star reviews.  The first one is from yelp and the second one is from amazon.  The idea here is that when someone gives a rating they're essentially labeling the sentiment of the review they leave behind.  That can be exploited to train a sentiment analysis NN model.  The data used here is sufficiently large while also being widely available on the internet for free via kaggle.\n",
        "\n",
        "As for the sarcasm training dataset, we're using reddit sarcastic comments as well as the onion headlines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEw3pNDNSAk5"
      },
      "source": [
        "### iii. Load Dataset (Load your dataset here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         5\n",
              "1         5\n",
              "2         4\n",
              "3         5\n",
              "4         5\n",
              "         ..\n",
              "568449    5\n",
              "568450    2\n",
              "568451    5\n",
              "568452    5\n",
              "568453    5\n",
              "Name: label, Length: 578454, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#SENTIMENT DATASET\n",
        "\n",
        "# Load the yelp review dataset from local file path\n",
        "df_yelp = pd.read_csv('data/yelp.csv')\n",
        "\n",
        "# Get rid of all the irrelevant columns that we won't be needing leaving behind just the content and the labels\n",
        "df_yelp.drop(df_yelp.columns[[0,1,2,5,6,7,8,9]], axis=1, inplace=True)\n",
        "\n",
        "# Renaming the two columns left to something descriptive\n",
        "df_yelp.columns = ['label', 'clean_comment']\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "# Load the amazon review dataset from local file path\n",
        "df_amazon = pd.read_csv('data/amazonreviews.csv')\n",
        "\n",
        "# Get rid of all the irrelevant columns that we won't be needing leaving behind just the content and the labels\n",
        "df_amazon.drop(df_amazon.columns[[0,1,2,3,4,5,7,8]], axis=1, inplace=True)\n",
        "\n",
        "# Renaming the two columns left to something descriptive\n",
        "df_amazon.columns = ['label', 'clean_comment']\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "# Mix the two review datasets\n",
        "df_sentiment = pd.concat([df_yelp, df_amazon])\n",
        "\n",
        "# Delete any rows with garbage labels\n",
        "df_sentiment = df_sentiment.drop(df_sentiment[(df_sentiment[\"label\"] != 1) & (df_sentiment[\"label\"] != 2) & (df_sentiment[\"label\"] != 3) & (df_sentiment[\"label\"] != 4) & (df_sentiment[\"label\"] != 5)].index)\n",
        "\n",
        "# Cast the labels as integers\n",
        "df_sentiment[\"label\"].astype('int')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "26704    0\n",
              "26705    0\n",
              "26706    0\n",
              "26707    0\n",
              "26708    0\n",
              "Name: label, Length: 1037535, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SARCASM DATASET\n",
        "\n",
        "# Load the reddit comments dataset from local file path\n",
        "df_reddit = pd.read_csv('data/train-balanced-sarcasm.csv')\n",
        "\n",
        "# Get rid of all the irrelevant columns that we won't be needing leaving behind just the content and the labels\n",
        "df_reddit.drop(df_reddit.columns[[2,3,4,5,6,7,8,9]], axis=1, inplace=True)\n",
        "\n",
        "# Renaming the two columns left to something descriptive\n",
        "df_reddit.columns = ['label','clean_comment']\n",
        "\n",
        "# Cast the labels as integers\n",
        "df_reddit[\"label\"].astype('int')\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "# Load the json onion headlines dataset from local file path\n",
        "df_onion = pd.read_json('archive/Sarcasm_Headlines_Dataset.json', lines = True)\n",
        "\n",
        "# Get rid of all the irrelevant columns that we won't be needing leaving behind just the content and the labels\n",
        "df_onion.drop(df_onion.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# Renaming the two columns left to something descriptive\n",
        "df_onion.columns = ['clean_comment','label']\n",
        "\n",
        "# Delete any rows with garbage labels\n",
        "df_onion = df_onion.drop(df_onion[(df_onion[\"label\"] != 0) & (df_onion[\"label\"] != 1)].index)\n",
        "\n",
        "# Cast the labels as integers\n",
        "df_onion[\"label\"].astype('int')\n",
        "\n",
        "# Mix the two sarcasm datasets into a master sarcasm dataframe\n",
        "df_sarcasm = pd.concat([df_reddit, df_onion])\n",
        "\n",
        "# Cast the labels as integers\n",
        "df_sarcasm[\"label\"].astype('int')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing the Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G4SAOYISGot"
      },
      "source": [
        "### i. Upsample Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The idea of upsampling is ensuring that all data labels in our dataset have the same count.  Naturally, you wouldn't expect randomly sampled comments and reviews from around the internet to contain a uniform distribution of labels, where usually positive sentiment statements are more common.  This phenomenon needs to be addressed in order to reduce bias in the training dataset.  To do that, we use upsampling which ensures all labels in our dataset have the same count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vg9IA_HKe20",
        "outputId": "6b0ae599-eef4-4037-dc24-222b6616a1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "majority class before upsample: (366459, 2)\n",
            "minority class before upsample: (30696, 2)\n",
            "minority class2 before upsample: (44101, 2)\n",
            "minority class3 before upsample: (84181, 2)\n",
            "minority class4 before upsample: (53017, 2)\n",
            "After upsampling\n",
            "label\n",
            "5    366459\n",
            "2    366459\n",
            "3    366459\n",
            "4    366459\n",
            "1    366459\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Separate the data based on their labels\n",
        "data_majority = df_sentiment[df_sentiment['label'] == 5]\n",
        "data_minority = df_sentiment[df_sentiment['label'] == 2]\n",
        "data_minority1 = df_sentiment[df_sentiment['label'] == 3]\n",
        "data_minority2 = df_sentiment[df_sentiment['label'] == 4]\n",
        "data_minority3 = df_sentiment[df_sentiment['label'] == 1]\n",
        "\n",
        "# Calculate bias\n",
        "bias = data_minority.shape[0]/data_majority.shape[0]\n",
        "\n",
        "# Mix around the data\n",
        "train = pd.concat([data_majority.sample(frac=1,random_state=200),\n",
        "         data_minority.sample(frac=1,random_state=200), data_minority1.sample(frac=1,random_state=200),  data_minority2.sample(frac=1,random_state=200), data_minority3.sample(frac=1,random_state=200)])\n",
        "\n",
        "#Seperate the data based on their labels again\n",
        "data_majority = train[train['label'] == 5]\n",
        "data_minority = train[train['label'] == 2]\n",
        "data_minority1 = train[train['label'] == 3]\n",
        "data_minority2 = train[train['label'] == 4]\n",
        "data_minority3 = train[train['label'] == 1]\n",
        "\n",
        "#show the count of different labels\n",
        "print(\"majority class before upsample:\",data_majority.shape)\n",
        "print(\"minority class before upsample:\",data_minority.shape)\n",
        "print(\"minority class2 before upsample:\",data_minority1.shape)\n",
        "print(\"minority class3 before upsample:\",data_minority2.shape)\n",
        "print(\"minority class4 before upsample:\",data_minority3.shape)\n",
        "\n",
        "# Upsample minority class\n",
        "data_minority_upsampled = resample(data_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "data_minority_upsampled1 = resample(data_minority1, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "data_minority_upsampled2 = resample(data_minority2, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "data_minority_upsampled3 = resample(data_minority3, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "data_upsampled_sentiment = pd.concat([data_majority, data_minority_upsampled, data_minority_upsampled1, data_minority_upsampled2, data_minority_upsampled3])\n",
        " \n",
        "# Display new class counts\n",
        "print(\"After upsampling\\n\",data_upsampled_sentiment.label.value_counts(),sep = \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRaXSas2acvS",
        "outputId": "4c12d0a4-b99d-4a13-8a1d-cc903d18c692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not sarcastic data in training: 520398\n",
            "sarcastic data in training: 517137\n",
            "majority class before upsample: (520398, 2)\n",
            "minority class before upsample: (517137, 2)\n",
            "After upsampling\n",
            "label\n",
            "0    520398\n",
            "1    520398\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Separate majority and minority classes\n",
        "data_majority = df_sarcasm[df_sarcasm['label'] == 0]\n",
        "data_minority = df_sarcasm[df_sarcasm['label'] == 1]\n",
        "\n",
        "# Calculate Bias in the data\n",
        "bias = data_minority.shape[0]/data_majority.shape[0]\n",
        "\n",
        "# Mix around the data\n",
        "train = pd.concat([data_majority.sample(frac=1,random_state=200),\n",
        "         data_minority.sample(frac=1,random_state=200)])\n",
        "\n",
        "# Print out the labeled data's count\n",
        "print('Not sarcastic data in training:',(train.label == 0).sum())\n",
        "print('sarcastic data in training:',(train.label == 1).sum())\n",
        "\n",
        "# Separate majority and minority classes in training data for upsampling \n",
        "data_majority = train[train['label'] == 0]\n",
        "data_minority = train[train['label'] == 1]\n",
        "\n",
        "\n",
        "print(\"majority class before upsample:\",data_majority.shape)\n",
        "print(\"minority class before upsample:\",data_minority.shape)\n",
        "\n",
        "# Upsample minority class\n",
        "data_minority_upsampled = resample(data_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "data_upsampled_sarcasm = pd.concat([data_majority, data_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n",
        "print(\"After upsampling\\n\",data_upsampled_sarcasm.label.value_counts(),sep = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxD4YDy5SPzA"
      },
      "source": [
        "**Visualize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "C-6TPDovaf3q",
        "outputId": "58981e75-aa2f-47b6-9f5b-f3dade672c25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='count', ylabel='label'>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.set_style(\"dark\")\n",
        "\n",
        "# plot how many labeled data there are from the upsampled data\n",
        "sns.countplot(data_upsampled_sentiment.label.astype('category'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "-nBe9aZRpvKb",
        "outputId": "6882a72b-c27b-4a31-f932-0e4a83503147"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='count', ylabel='label'>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot how many labeled data there are from the upsampled data\n",
        "\n",
        "sns.countplot(data_upsampled_sarcasm.label.astype('category'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X11tK0Q2STtx"
      },
      "source": [
        "**Data Pre-process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TirRj2faf3r"
      },
      "outputs": [],
      "source": [
        "\n",
        "def depure_data(data):\n",
        "    #Removing URLs with a regular expression\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    data = url_pattern.sub(r'', data)\n",
        "\n",
        "    # Remove Emails\n",
        "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "\n",
        "    # Remove new line characters\n",
        "    data = re.sub('\\s+', ' ', data)\n",
        "\n",
        "    # Remove distracting single quotes\n",
        "    data = re.sub(\"\\'\", \"\", data)\n",
        "        \n",
        "    return data\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations\n",
        "\n",
        "def preprocess(tweet, remove_stopwords=True, lemmatize=True):\n",
        "    temp = [] \n",
        "    # Splitting pd.Series to list\n",
        "    data_to_list = tweet['clean_comment'].values.tolist()\n",
        "    for i in range(len(data_to_list)):\n",
        "        temp.append(depure_data(data_to_list[i]))  \n",
        "    data_words = list(sent_to_words(temp))\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        data_words = [[word for word in doc if word not in gensim.parsing.preprocessing.STOPWORDS] for doc in data_words]\n",
        "    \n",
        "    # Lemmatize words\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        data_words = [[lemmatizer.lemmatize(word) for word in doc] for doc in data_words]\n",
        "        \n",
        "    data = []\n",
        "    for i in range(len(data_words)):\n",
        "        data.append(TreebankWordDetokenizer().detokenize(data_words[i]))\n",
        "    del data_words, temp, i\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>clean_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>551146</th>\n",
              "      <td>5</td>\n",
              "      <td>These are raw, whole, natural, cashews. Much l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341934</th>\n",
              "      <td>5</td>\n",
              "      <td>We were the grind your own beans, french press...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260284</th>\n",
              "      <td>5</td>\n",
              "      <td>I have had celiac for over 7 years.  I found o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63655</th>\n",
              "      <td>5</td>\n",
              "      <td>Okay I am not a hot coffee person. But iced co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199091</th>\n",
              "      <td>5</td>\n",
              "      <td>I have a three year old and a seven year old a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217156</th>\n",
              "      <td>1</td>\n",
              "      <td>I have been drinking Pero ever since I was a l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402342</th>\n",
              "      <td>1</td>\n",
              "      <td>Well, it looks like they took my review off fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51701</th>\n",
              "      <td>1</td>\n",
              "      <td>I ordered this lavender to use in ice cream. B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446020</th>\n",
              "      <td>1</td>\n",
              "      <td>There is very little meat in this food and the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483488</th>\n",
              "      <td>1</td>\n",
              "      <td>I had expected more but I chalk it up to my no...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1832295 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                      clean_comment\n",
              "551146      5  These are raw, whole, natural, cashews. Much l...\n",
              "341934      5  We were the grind your own beans, french press...\n",
              "260284      5  I have had celiac for over 7 years.  I found o...\n",
              "63655       5  Okay I am not a hot coffee person. But iced co...\n",
              "199091      5  I have a three year old and a seven year old a...\n",
              "...       ...                                                ...\n",
              "217156      1  I have been drinking Pero ever since I was a l...\n",
              "402342      1  Well, it looks like they took my review off fr...\n",
              "51701       1  I ordered this lavender to use in ice cream. B...\n",
              "446020      1  There is very little meat in this food and the...\n",
              "483488      1  I had expected more but I chalk it up to my no...\n",
              "\n",
              "[1832295 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#import string type casting library\n",
        "from pandas.api.types import is_string_dtype\n",
        "\n",
        "display(data_upsampled_sentiment)\n",
        "\n",
        "#cast all text as string to avoid any problems in processing the data later\n",
        "data_upsampled_sentiment['clean_comment'] = data_upsampled_sentiment['clean_comment'].astype(str)\n",
        "data_upsampled_sarcasm['clean_comment'] = data_upsampled_sarcasm['clean_comment'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofb6SYZuaf3s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/Ahmad/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#run the text data through the previously defined preprocessing function\n",
        "#this removes commas, periods and other useless symbols as well as links\n",
        "#keeping only meaningful words\n",
        "sarcasm_data = preprocess(data_upsampled_sarcasm)\n",
        "sentiment_data = preprocess(data_upsampled_sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46wPcyQSYdc"
      },
      "source": [
        "**Tokenize word**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJlTpKCvaf3s",
        "outputId": "29c2a552-f090-4327-eec4-6c59e45ea69f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[   0,    0,    0, ...,   23,  125,  108],\n",
              "        [   0,    0,    0, ..., 1108,  125,  108],\n",
              "        [   0,    0,    0, ...,   68,   38,   48],\n",
              "        ...,\n",
              "        [   0,    0,    0, ...,  390, 1084,  501],\n",
              "        [   0,    0,    0, ...,    8,  222,  533],\n",
              "        [   0,    0,    0, ...,  121,  125,  538]], dtype=int32),\n",
              " array([[   0,    0,    0, ...,    0,   69,   22],\n",
              "        [   0,    0,    0, ...,  217,   93, 1317],\n",
              "        [   0,    0,    0, ...,    0,    0,   22],\n",
              "        ...,\n",
              "        [   0,    0,    0, ..., 1346, 1818,  128],\n",
              "        [   0,    0,    0, ...,    0,    0,   86],\n",
              "        [   0,    0,    0, ...,   57,   24, 2159]], dtype=int32))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#max amount of words in our tokenizer library\n",
        "max_words = 5000\n",
        "\n",
        "#maximum number of words in a sentence\n",
        "max_len = 200\n",
        "\n",
        "data_sentiment = sentiment_data\n",
        "\n",
        "#instantiate the tokenizer object\n",
        "tokenizer_sentiment = Tokenizer(num_words=max_words)\n",
        "\n",
        "#create a tokenized library based on the sentiment data\n",
        "#this creates an indexed list library of the top 5000 most common\n",
        "#words in the data_sentiment variable\n",
        "tokenizer_sentiment.fit_on_texts(data_sentiment)\n",
        "\n",
        "#turns all the sentences in the data_sentiment variable into numbers\n",
        "#based on their index in the tokenizer object\n",
        "sequences_sentiment = tokenizer_sentiment.texts_to_sequences(data_sentiment)\n",
        "\n",
        "#this adds 0s as filler to pad each sentence to make it \"200\" words even if the sentences \n",
        "# are under 200 words because the NN architecture expects 200 word sentences \n",
        "tweets_sentiment = pad_sequences(sequences_sentiment, maxlen=max_len, truncating='post')\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "\n",
        "data_sarcasm = sarcasm_data\n",
        "\n",
        "#instantiate the tokenizer object\n",
        "tokenizer_sarcasm = Tokenizer(num_words=max_words)\n",
        "\n",
        "#create a tokenized library based on the sarcasm data\n",
        "#this creates an indexed list library of the top 5000 most common\n",
        "#words in the data_sarcasm variable\n",
        "tokenizer_sarcasm.fit_on_texts(data_sarcasm)\n",
        "\n",
        "#turns all the sentences in the data_sarcasm variable into numbers\n",
        "#based on their index in the tokenizer object\n",
        "sequences_sarcasm = tokenizer_sarcasm.texts_to_sequences(data_sarcasm)\n",
        "\n",
        "#this adds 0s as filler to pad each sentence to make it \"200\" words even if the sentences \n",
        "# are under 200 words because the NN architecture expects 200 word sentences \n",
        "tweets_sarcasm = pad_sequences(sequences_sarcasm, maxlen=max_len, truncating='post')\n",
        "\n",
        "tweets_sentiment, tweets_sarcasm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,  550,  109,\n",
              "       1174,    2,  550,  307,  598,  667,  232,    1,    1, 2737,  667,\n",
              "       1174,  108,  742,  389,   12,  423,  667,  105,  106,    3, 1174,\n",
              "        495,    3,    2,    1,    1,  324, 1174,   87,    2,   33,   23,\n",
              "        125,  108], dtype=int32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'br': 1,\n",
              " 'like': 2,\n",
              " 'taste': 3,\n",
              " 'product': 4,\n",
              " 'good': 5,\n",
              " 'flavor': 6,\n",
              " 'coffee': 7,\n",
              " 'food': 8,\n",
              " 'tea': 9,\n",
              " 'great': 10,\n",
              " 'dont': 11,\n",
              " 'love': 12,\n",
              " 'dog': 13,\n",
              " 'time': 14,\n",
              " 'cup': 15,\n",
              " 'bag': 16,\n",
              " 'little': 17,\n",
              " 'im': 18,\n",
              " 'buy': 19,\n",
              " 'box': 20,\n",
              " 'better': 21,\n",
              " 'amazon': 22,\n",
              " 'price': 23,\n",
              " 'use': 24,\n",
              " 'tried': 25,\n",
              " 'try': 26,\n",
              " 'water': 27,\n",
              " 'eat': 28,\n",
              " 'ive': 29,\n",
              " 'chocolate': 30,\n",
              " 'drink': 31,\n",
              " 'cat': 32,\n",
              " 'think': 33,\n",
              " 'sugar': 34,\n",
              " 'day': 35,\n",
              " 'way': 36,\n",
              " 'treat': 37,\n",
              " 'brand': 38,\n",
              " 'bought': 39,\n",
              " 'ingredient': 40,\n",
              " 'thing': 41,\n",
              " 'didnt': 42,\n",
              " 'sweet': 43,\n",
              " 'know': 44,\n",
              " 'store': 45,\n",
              " 'bit': 46,\n",
              " 'got': 47,\n",
              " 'best': 48,\n",
              " 'bad': 49,\n",
              " 'review': 50,\n",
              " 'order': 51,\n",
              " 'year': 52,\n",
              " 'want': 53,\n",
              " 'lot': 54,\n",
              " 'bar': 55,\n",
              " 'thought': 56,\n",
              " 'pack': 57,\n",
              " 'free': 58,\n",
              " 'star': 59,\n",
              " 'chip': 60,\n",
              " 'package': 61,\n",
              " 'doesnt': 62,\n",
              " 'work': 63,\n",
              " 'ordered': 64,\n",
              " 'nice': 65,\n",
              " 'mix': 66,\n",
              " 'come': 67,\n",
              " 'different': 68,\n",
              " 'sure': 69,\n",
              " 'smell': 70,\n",
              " 'small': 71,\n",
              " 'need': 72,\n",
              " 'quality': 73,\n",
              " 'pretty': 74,\n",
              " 'stuff': 75,\n",
              " 'hot': 76,\n",
              " 'old': 77,\n",
              " 'organic': 78,\n",
              " 'strong': 79,\n",
              " 'recommend': 80,\n",
              " 'hard': 81,\n",
              " 'problem': 82,\n",
              " 'eating': 83,\n",
              " 'bottle': 84,\n",
              " 'snack': 85,\n",
              " 'right': 86,\n",
              " 'look': 87,\n",
              " 'green': 88,\n",
              " 'add': 89,\n",
              " 'going': 90,\n",
              " 'people': 91,\n",
              " 'milk': 92,\n",
              " 'oil': 93,\n",
              " 'looking': 94,\n",
              " 'chicken': 95,\n",
              " 'high': 96,\n",
              " 'tasted': 97,\n",
              " 'calorie': 98,\n",
              " 'item': 99,\n",
              " 'size': 100,\n",
              " 'month': 101,\n",
              " 'favorite': 102,\n",
              " 'buying': 103,\n",
              " 'make': 104,\n",
              " 'salt': 105,\n",
              " 'actually': 106,\n",
              " 'cooky': 107,\n",
              " 'disappointed': 108,\n",
              " 'natural': 109,\n",
              " 'kind': 110,\n",
              " 'new': 111,\n",
              " 'regular': 112,\n",
              " 'sauce': 113,\n",
              " 'healthy': 114,\n",
              " 'long': 115,\n",
              " 'maybe': 116,\n",
              " 'said': 117,\n",
              " 'big': 118,\n",
              " 'away': 119,\n",
              " 'probably': 120,\n",
              " 'definitely': 121,\n",
              " 'thats': 122,\n",
              " 'feel': 123,\n",
              " 'real': 124,\n",
              " 'wont': 125,\n",
              " 'id': 126,\n",
              " 'money': 127,\n",
              " 'texture': 128,\n",
              " 'purchased': 129,\n",
              " 'oz': 130,\n",
              " 'company': 131,\n",
              " 'received': 132,\n",
              " 'purchase': 133,\n",
              " 'dry': 134,\n",
              " 'diet': 135,\n",
              " 'enjoy': 136,\n",
              " 'fresh': 137,\n",
              " 'half': 138,\n",
              " 'far': 139,\n",
              " 'ill': 140,\n",
              " 'isnt': 141,\n",
              " 'trying': 142,\n",
              " 'bean': 143,\n",
              " 'shipping': 144,\n",
              " 'delicious': 145,\n",
              " 'fat': 146,\n",
              " 'piece': 147,\n",
              " 'fruit': 148,\n",
              " 'case': 149,\n",
              " 'worth': 150,\n",
              " 'week': 151,\n",
              " 'tasty': 152,\n",
              " 'local': 153,\n",
              " 'expensive': 154,\n",
              " 'came': 155,\n",
              " 'wasnt': 156,\n",
              " 'variety': 157,\n",
              " 'blend': 158,\n",
              " 'instead': 159,\n",
              " 'tasting': 160,\n",
              " 'candy': 161,\n",
              " 'butter': 162,\n",
              " 'easy': 163,\n",
              " 'getting': 164,\n",
              " 'fine': 165,\n",
              " 'cereal': 166,\n",
              " 'dark': 167,\n",
              " 'packaging': 168,\n",
              " 'low': 169,\n",
              " 'flavored': 170,\n",
              " 'place': 171,\n",
              " 'peanut': 172,\n",
              " 'rice': 173,\n",
              " 'vanilla': 174,\n",
              " 'coconut': 175,\n",
              " 'cost': 176,\n",
              " 'added': 177,\n",
              " 'youre': 178,\n",
              " 'bitter': 179,\n",
              " 'having': 180,\n",
              " 'usually': 181,\n",
              " 'gave': 182,\n",
              " 'ounce': 183,\n",
              " 'happy': 184,\n",
              " 'serving': 185,\n",
              " 'ok': 186,\n",
              " 'minute': 187,\n",
              " 'wanted': 188,\n",
              " 'protein': 189,\n",
              " 'corn': 190,\n",
              " 'stick': 191,\n",
              " 'pod': 192,\n",
              " 'help': 193,\n",
              " 'meal': 194,\n",
              " 'fact': 195,\n",
              " 'second': 196,\n",
              " 'cheese': 197,\n",
              " 'perfect': 198,\n",
              " 'black': 199,\n",
              " 'theyre': 200,\n",
              " 'gluten': 201,\n",
              " 'roast': 202,\n",
              " 'read': 203,\n",
              " 'one': 204,\n",
              " 'can': 205,\n",
              " 'light': 206,\n",
              " 'took': 207,\n",
              " 'large': 208,\n",
              " 'open': 209,\n",
              " 'liked': 210,\n",
              " 'juice': 211,\n",
              " 'kid': 212,\n",
              " 'went': 213,\n",
              " 'grocery': 214,\n",
              " 'arrived': 215,\n",
              " 'prefer': 216,\n",
              " 'drinking': 217,\n",
              " 'wouldnt': 218,\n",
              " 'type': 219,\n",
              " 'baby': 220,\n",
              " 'let': 221,\n",
              " 'meat': 222,\n",
              " 'morning': 223,\n",
              " 'syrup': 224,\n",
              " 'hour': 225,\n",
              " 'health': 226,\n",
              " 'decided': 227,\n",
              " 'making': 228,\n",
              " 'opened': 229,\n",
              " 'loved': 230,\n",
              " 'home': 231,\n",
              " 'nut': 232,\n",
              " 'reason': 233,\n",
              " 'extra': 234,\n",
              " 'friend': 235,\n",
              " 'guess': 236,\n",
              " 'packet': 237,\n",
              " 'plastic': 238,\n",
              " 'end': 239,\n",
              " 'powder': 240,\n",
              " 'wish': 241,\n",
              " 'soup': 242,\n",
              " 'chew': 243,\n",
              " 'href': 244,\n",
              " 'white': 245,\n",
              " 'save': 246,\n",
              " 'couple': 247,\n",
              " 'especially': 248,\n",
              " 'unfortunately': 249,\n",
              " 'able': 250,\n",
              " 'family': 251,\n",
              " 'potato': 252,\n",
              " 'date': 253,\n",
              " 'mouth': 254,\n",
              " 'honey': 255,\n",
              " 'deal': 256,\n",
              " 'color': 257,\n",
              " 'house': 258,\n",
              " 'cookie': 259,\n",
              " 'spice': 260,\n",
              " 'ground': 261,\n",
              " 'fan': 262,\n",
              " 'issue': 263,\n",
              " 'container': 264,\n",
              " 'overall': 265,\n",
              " 'started': 266,\n",
              " 'cream': 267,\n",
              " 'wonderful': 268,\n",
              " 'excellent': 269,\n",
              " 'left': 270,\n",
              " 'energy': 271,\n",
              " 'jerky': 272,\n",
              " 'available': 273,\n",
              " 'aftertaste': 274,\n",
              " 'popcorn': 275,\n",
              " 'leaf': 276,\n",
              " 'say': 277,\n",
              " 'weak': 278,\n",
              " 'artificial': 279,\n",
              " 'cheaper': 280,\n",
              " 'hair': 281,\n",
              " 'care': 282,\n",
              " 'vitamin': 283,\n",
              " 'version': 284,\n",
              " 'bold': 285,\n",
              " 'starbucks': 286,\n",
              " 'list': 287,\n",
              " 'brew': 288,\n",
              " 'mean': 289,\n",
              " 'formula': 290,\n",
              " 'grain': 291,\n",
              " 'husband': 292,\n",
              " 'bite': 293,\n",
              " 'pet': 294,\n",
              " 'experience': 295,\n",
              " 'hand': 296,\n",
              " 'jar': 297,\n",
              " 'red': 298,\n",
              " 'ago': 299,\n",
              " 'breakfast': 300,\n",
              " 'label': 301,\n",
              " 'plus': 302,\n",
              " 'beef': 303,\n",
              " 'service': 304,\n",
              " 'ginger': 305,\n",
              " 'tell': 306,\n",
              " 'almond': 307,\n",
              " 'recipe': 308,\n",
              " 'longer': 309,\n",
              " 'wrong': 310,\n",
              " 'expected': 311,\n",
              " 'machine': 312,\n",
              " 'idea': 313,\n",
              " 'salty': 314,\n",
              " 'cracker': 315,\n",
              " 'choice': 316,\n",
              " 'soft': 317,\n",
              " 'inside': 318,\n",
              " 'keurig': 319,\n",
              " 'dried': 320,\n",
              " 'ordering': 321,\n",
              " 'highly': 322,\n",
              " 'quickly': 323,\n",
              " 'pound': 324,\n",
              " 'hope': 325,\n",
              " 'change': 326,\n",
              " 'gift': 327,\n",
              " 'mixed': 328,\n",
              " 'spicy': 329,\n",
              " 'bread': 330,\n",
              " 'reviewer': 331,\n",
              " 'original': 332,\n",
              " 'cut': 333,\n",
              " 'apple': 334,\n",
              " 'bowl': 335,\n",
              " 'slightly': 336,\n",
              " 'looked': 337,\n",
              " 'result': 338,\n",
              " 'line': 339,\n",
              " 'difference': 340,\n",
              " 'brown': 341,\n",
              " 'absolutely': 342,\n",
              " 'believe': 343,\n",
              " 'waste': 344,\n",
              " 'customer': 345,\n",
              " 'single': 346,\n",
              " 'sweetener': 347,\n",
              " 'cinnamon': 348,\n",
              " 'soda': 349,\n",
              " 'there': 350,\n",
              " 'ate': 351,\n",
              " 'giving': 352,\n",
              " 'pay': 353,\n",
              " 'recommended': 354,\n",
              " 'noodle': 355,\n",
              " 'weight': 356,\n",
              " 'content': 357,\n",
              " 'similar': 358,\n",
              " 'start': 359,\n",
              " 'point': 360,\n",
              " 'contains': 361,\n",
              " 'smaller': 362,\n",
              " 'noticed': 363,\n",
              " 'lemon': 364,\n",
              " 'value': 365,\n",
              " 'wheat': 366,\n",
              " 'lb': 367,\n",
              " 'smooth': 368,\n",
              " 'based': 369,\n",
              " 'feed': 370,\n",
              " 'alternative': 371,\n",
              " 'life': 372,\n",
              " 'past': 373,\n",
              " 'sodium': 374,\n",
              " 'description': 375,\n",
              " 'return': 376,\n",
              " 'okay': 377,\n",
              " 'night': 378,\n",
              " 'pepper': 379,\n",
              " 'french': 380,\n",
              " 'seed': 381,\n",
              " 'cheap': 382,\n",
              " 'son': 383,\n",
              " 'cake': 384,\n",
              " 'pasta': 385,\n",
              " 'canned': 386,\n",
              " 'expect': 387,\n",
              " 'close': 388,\n",
              " 'stale': 389,\n",
              " 'non': 390,\n",
              " 'market': 391,\n",
              " 'enjoyed': 392,\n",
              " 'batch': 393,\n",
              " 'cocoa': 394,\n",
              " 'excited': 395,\n",
              " 'saw': 396,\n",
              " 'plain': 397,\n",
              " 'soy': 398,\n",
              " 'decent': 399,\n",
              " 'special': 400,\n",
              " 'get': 401,\n",
              " 'super': 402,\n",
              " 'orange': 403,\n",
              " 'gum': 404,\n",
              " 'medium': 405,\n",
              " 'crunchy': 406,\n",
              " 'rich': 407,\n",
              " 'clean': 408,\n",
              " 'youll': 409,\n",
              " 'rest': 410,\n",
              " 'quick': 411,\n",
              " 'given': 412,\n",
              " 'fast': 413,\n",
              " 'yes': 414,\n",
              " 'bland': 415,\n",
              " 'later': 416,\n",
              " 'compared': 417,\n",
              " 'awful': 418,\n",
              " 'easily': 419,\n",
              " 'course': 420,\n",
              " 'huge': 421,\n",
              " 'toy': 422,\n",
              " 'arent': 423,\n",
              " 'completely': 424,\n",
              " 'note': 425,\n",
              " 'called': 426,\n",
              " 'opinion': 427,\n",
              " 'mind': 428,\n",
              " 'horrible': 429,\n",
              " 'aroma': 430,\n",
              " 'havent': 431,\n",
              " 'flour': 432,\n",
              " 'gram': 433,\n",
              " 'run': 434,\n",
              " 'teeth': 435,\n",
              " 'terrible': 436,\n",
              " 'certainly': 437,\n",
              " 'ice': 438,\n",
              " 'cold': 439,\n",
              " 'fiber': 440,\n",
              " 'online': 441,\n",
              " 'mild': 442,\n",
              " 'bone': 443,\n",
              " 'blue': 444,\n",
              " 'hoping': 445,\n",
              " 'exactly': 446,\n",
              " 'option': 447,\n",
              " 'dish': 448,\n",
              " 'chemical': 449,\n",
              " 'sell': 450,\n",
              " 'caffeine': 451,\n",
              " 'go': 452,\n",
              " 'told': 453,\n",
              " 'thinking': 454,\n",
              " 'feeding': 455,\n",
              " 'daughter': 456,\n",
              " 'glass': 457,\n",
              " 'fish': 458,\n",
              " 'recently': 459,\n",
              " 'picture': 460,\n",
              " 'simply': 461,\n",
              " 'pop': 462,\n",
              " 'stomach': 463,\n",
              " 'surprised': 464,\n",
              " 'liquid': 465,\n",
              " 'maker': 466,\n",
              " 'vet': 467,\n",
              " 'finally': 468,\n",
              " 'amazing': 469,\n",
              " 'sent': 470,\n",
              " 'decaf': 471,\n",
              " 'wife': 472,\n",
              " 'leave': 473,\n",
              " 'adding': 474,\n",
              " 'expecting': 475,\n",
              " 'gone': 476,\n",
              " 'extremely': 477,\n",
              " 'twice': 478,\n",
              " 'sort': 479,\n",
              " 'normal': 480,\n",
              " 'instant': 481,\n",
              " 'broken': 482,\n",
              " 'felt': 483,\n",
              " 'espresso': 484,\n",
              " 'ball': 485,\n",
              " 'reading': 486,\n",
              " 'body': 487,\n",
              " 'sale': 488,\n",
              " 'worst': 489,\n",
              " 'cooking': 490,\n",
              " 'mountain': 491,\n",
              " 'count': 492,\n",
              " 'cherry': 493,\n",
              " 'packaged': 494,\n",
              " 'supposed': 495,\n",
              " 'salad': 496,\n",
              " 'shipment': 497,\n",
              " 'stay': 498,\n",
              " 'sour': 499,\n",
              " 'chewy': 500,\n",
              " 'total': 501,\n",
              " 'contain': 502,\n",
              " 'seller': 503,\n",
              " 'true': 504,\n",
              " 'larger': 505,\n",
              " 'granola': 506,\n",
              " 'china': 507,\n",
              " 'benefit': 508,\n",
              " 'daily': 509,\n",
              " 'entire': 510,\n",
              " 'effect': 511,\n",
              " 'hint': 512,\n",
              " 'changed': 513,\n",
              " 'turn': 514,\n",
              " 'tiny': 515,\n",
              " 'oatmeal': 516,\n",
              " 'restaurant': 517,\n",
              " 'needed': 518,\n",
              " 'matter': 519,\n",
              " 'cook': 520,\n",
              " 'cause': 521,\n",
              " 'finish': 522,\n",
              " 'heat': 523,\n",
              " 'today': 524,\n",
              " 'olive': 525,\n",
              " 'eaten': 526,\n",
              " 'glad': 527,\n",
              " 'vegetable': 528,\n",
              " 'normally': 529,\n",
              " 'throw': 530,\n",
              " 'strawberry': 531,\n",
              " 'pot': 532,\n",
              " 'flavoring': 533,\n",
              " 'live': 534,\n",
              " 'listed': 535,\n",
              " 'soon': 536,\n",
              " 'pouch': 537,\n",
              " 'purchasing': 538,\n",
              " 'lunch': 539,\n",
              " 'shipped': 540,\n",
              " 'particular': 541,\n",
              " 'acid': 542,\n",
              " 'filling': 543,\n",
              " 'chai': 544,\n",
              " 'ended': 545,\n",
              " 'brewed': 546,\n",
              " 'flavorful': 547,\n",
              " 'stop': 548,\n",
              " 'dinner': 549,\n",
              " 'raw': 550,\n",
              " 'child': 551,\n",
              " 'somewhat': 552,\n",
              " 'mess': 553,\n",
              " 'egg': 554,\n",
              " 'rating': 555,\n",
              " 'break': 556,\n",
              " 'check': 557,\n",
              " 'consistency': 558,\n",
              " 'taking': 559,\n",
              " 'berry': 560,\n",
              " 'loose': 561,\n",
              " 'seasoning': 562,\n",
              " 'dollar': 563,\n",
              " 'level': 564,\n",
              " 'picky': 565,\n",
              " 'avoid': 566,\n",
              " 'sold': 567,\n",
              " 'difficult': 568,\n",
              " 'mint': 569,\n",
              " 'website': 570,\n",
              " 'shop': 571,\n",
              " 'take': 572,\n",
              " 'stuck': 573,\n",
              " 'state': 574,\n",
              " 'sweetness': 575,\n",
              " 'clear': 576,\n",
              " 'delivery': 577,\n",
              " 'hate': 578,\n",
              " 'set': 579,\n",
              " 'puppy': 580,\n",
              " 'tomato': 581,\n",
              " 'notice': 582,\n",
              " 'short': 583,\n",
              " 'paid': 584,\n",
              " 'person': 585,\n",
              " 'convenient': 586,\n",
              " 'number': 587,\n",
              " 'iced': 588,\n",
              " 'worse': 589,\n",
              " 'stevia': 590,\n",
              " 'nearly': 591,\n",
              " 'wait': 592,\n",
              " 'caramel': 593,\n",
              " 'bulk': 594,\n",
              " 'seen': 595,\n",
              " 'blueberry': 596,\n",
              " 'area': 597,\n",
              " 'crunch': 598,\n",
              " 'disappointing': 599,\n",
              " 'hold': 600,\n",
              " 'oh': 601,\n",
              " 'higher': 602,\n",
              " 'sorry': 603,\n",
              " 'fit': 604,\n",
              " 'yummy': 605,\n",
              " 'stronger': 606,\n",
              " 'expiration': 607,\n",
              " 'offer': 608,\n",
              " 'claim': 609,\n",
              " 'actual': 610,\n",
              " 'touch': 611,\n",
              " 'immediately': 612,\n",
              " 'salmon': 613,\n",
              " 'word': 614,\n",
              " 'healthier': 615,\n",
              " 'anymore': 616,\n",
              " 'brewing': 617,\n",
              " 'thanks': 618,\n",
              " 'complaint': 619,\n",
              " 'eats': 620,\n",
              " 'packed': 621,\n",
              " 'subscribe': 622,\n",
              " 'stock': 623,\n",
              " 'standard': 624,\n",
              " 'pure': 625,\n",
              " 'skin': 626,\n",
              " 'pick': 627,\n",
              " 'weird': 628,\n",
              " 'lower': 629,\n",
              " 'wellness': 630,\n",
              " 'beverage': 631,\n",
              " 'continue': 632,\n",
              " 'veggie': 633,\n",
              " 'pleasant': 634,\n",
              " 'world': 635,\n",
              " 'cooked': 636,\n",
              " 'carry': 637,\n",
              " 'awesome': 638,\n",
              " 'tuna': 639,\n",
              " 'hit': 640,\n",
              " 'mg': 641,\n",
              " 'allergy': 642,\n",
              " 'poor': 643,\n",
              " 'licorice': 644,\n",
              " 'fairly': 645,\n",
              " 'fair': 646,\n",
              " 'give': 647,\n",
              " 'totally': 648,\n",
              " 'chili': 649,\n",
              " 'basically': 650,\n",
              " 'thank': 651,\n",
              " 'nutrition': 652,\n",
              " 'oat': 653,\n",
              " 'pleased': 654,\n",
              " 'double': 655,\n",
              " 'outside': 656,\n",
              " 'figured': 657,\n",
              " 'style': 658,\n",
              " 'remember': 659,\n",
              " 'fun': 660,\n",
              " 'paper': 661,\n",
              " 'source': 662,\n",
              " 'stash': 663,\n",
              " 'vinegar': 664,\n",
              " 'feeling': 665,\n",
              " 'plant': 666,\n",
              " 'roasted': 667,\n",
              " 'consider': 668,\n",
              " 'roll': 669,\n",
              " 'substitute': 670,\n",
              " 'chunk': 671,\n",
              " 'sick': 672,\n",
              " 'pumpkin': 673,\n",
              " 'turned': 674,\n",
              " 'threw': 675,\n",
              " 'stand': 676,\n",
              " 'portion': 677,\n",
              " 'worked': 678,\n",
              " 'sample': 679,\n",
              " 'lack': 680,\n",
              " 'manufacturer': 681,\n",
              " 'drinker': 682,\n",
              " 'test': 683,\n",
              " 'near': 684,\n",
              " 'stopped': 685,\n",
              " 'update': 686,\n",
              " 'nasty': 687,\n",
              " 'average': 688,\n",
              " 'brewer': 689,\n",
              " 'creamy': 690,\n",
              " 'addition': 691,\n",
              " 'garlic': 692,\n",
              " 'smelled': 693,\n",
              " 'room': 694,\n",
              " 'opening': 695,\n",
              " 'supermarket': 696,\n",
              " 'saying': 697,\n",
              " 'impressed': 698,\n",
              " 'future': 699,\n",
              " 'mistake': 700,\n",
              " 'individual': 701,\n",
              " 'lime': 702,\n",
              " 'direction': 703,\n",
              " 'including': 704,\n",
              " 'shelf': 705,\n",
              " 'understand': 706,\n",
              " 'crazy': 707,\n",
              " 'wet': 708,\n",
              " 'com': 709,\n",
              " 'personally': 710,\n",
              " 'pill': 711,\n",
              " 'instruction': 712,\n",
              " 'chewing': 713,\n",
              " 'nutritional': 714,\n",
              " 'heavy': 715,\n",
              " 'serve': 716,\n",
              " 'shake': 717,\n",
              " 'hazelnut': 718,\n",
              " 'newman': 719,\n",
              " 'raspberry': 720,\n",
              " 'kitchen': 721,\n",
              " 'baked': 722,\n",
              " 'baking': 723,\n",
              " 'banana': 724,\n",
              " 'form': 725,\n",
              " 'putting': 726,\n",
              " 'sitting': 727,\n",
              " 'agree': 728,\n",
              " 'truly': 729,\n",
              " 'sandwich': 730,\n",
              " 'onion': 731,\n",
              " 'information': 732,\n",
              " 'switch': 733,\n",
              " 'quantity': 734,\n",
              " 'solid': 735,\n",
              " 'extract': 736,\n",
              " 'asked': 737,\n",
              " 'hungry': 738,\n",
              " 'animal': 739,\n",
              " 'site': 740,\n",
              " 'mango': 741,\n",
              " 'likely': 742,\n",
              " 'weve': 743,\n",
              " 'table': 744,\n",
              " 'rate': 745,\n",
              " 'simple': 746,\n",
              " 'priced': 747,\n",
              " 'control': 748,\n",
              " 'premium': 749,\n",
              " 'combination': 750,\n",
              " 'plan': 751,\n",
              " 'science': 752,\n",
              " 'research': 753,\n",
              " 'compare': 754,\n",
              " 'process': 755,\n",
              " 'usual': 756,\n",
              " 'microwave': 757,\n",
              " 'barely': 758,\n",
              " 'replacement': 759,\n",
              " 'chance': 760,\n",
              " 'sealed': 761,\n",
              " 'cool': 762,\n",
              " 'air': 763,\n",
              " 'known': 764,\n",
              " 'fantastic': 765,\n",
              " 'condition': 766,\n",
              " 'coming': 767,\n",
              " 'disappointment': 768,\n",
              " 'sound': 769,\n",
              " 'strange': 770,\n",
              " 'drop': 771,\n",
              " 'christmas': 772,\n",
              " 'suggest': 773,\n",
              " 'job': 774,\n",
              " 'shampoo': 775,\n",
              " 'positive': 776,\n",
              " 'send': 777,\n",
              " 'imagine': 778,\n",
              " 'heard': 779,\n",
              " 'delivered': 780,\n",
              " 'kick': 781,\n",
              " 'lid': 782,\n",
              " 'adult': 783,\n",
              " 'turkey': 784,\n",
              " 'refund': 785,\n",
              " 'easier': 786,\n",
              " 'odd': 787,\n",
              " 'peach': 788,\n",
              " 'yellow': 789,\n",
              " 'forward': 790,\n",
              " 'gross': 791,\n",
              " 'lost': 792,\n",
              " 'kept': 793,\n",
              " 'supplement': 794,\n",
              " 'bacon': 795,\n",
              " 'bring': 796,\n",
              " 'carbs': 797,\n",
              " 'shot': 798,\n",
              " 'main': 799,\n",
              " 'gas': 800,\n",
              " 'mentioned': 801,\n",
              " 'working': 802,\n",
              " 'wonder': 803,\n",
              " 'guy': 804,\n",
              " 'figure': 805,\n",
              " 'splenda': 806,\n",
              " 'fall': 807,\n",
              " 'possible': 808,\n",
              " 'general': 809,\n",
              " 'warm': 810,\n",
              " 'beer': 811,\n",
              " 'pizza': 812,\n",
              " 'kibble': 813,\n",
              " 'straight': 814,\n",
              " 'considering': 815,\n",
              " 'pie': 816,\n",
              " 'cardboard': 817,\n",
              " 'italian': 818,\n",
              " 'filled': 819,\n",
              " 'he': 820,\n",
              " 'discovered': 821,\n",
              " 'receive': 822,\n",
              " 'overly': 823,\n",
              " 'us': 824,\n",
              " 'lover': 825,\n",
              " 'honestly': 826,\n",
              " 'crisp': 827,\n",
              " 'werent': 828,\n",
              " 'realize': 829,\n",
              " 'slight': 830,\n",
              " 'paying': 831,\n",
              " 'whats': 832,\n",
              " 'wrapped': 833,\n",
              " 'dented': 834,\n",
              " 'unlike': 835,\n",
              " 'human': 836,\n",
              " 'carb': 837,\n",
              " 'sized': 838,\n",
              " 'blood': 839,\n",
              " 'gotten': 840,\n",
              " 'ship': 841,\n",
              " 'scent': 842,\n",
              " 'offered': 843,\n",
              " 'knew': 844,\n",
              " 'bunch': 845,\n",
              " 'generally': 846,\n",
              " 'sensitive': 847,\n",
              " 'beware': 848,\n",
              " 'obviously': 849,\n",
              " 'mention': 850,\n",
              " 'important': 851,\n",
              " 'pricey': 852,\n",
              " 'refreshing': 853,\n",
              " 'tin': 854,\n",
              " 'watery': 855,\n",
              " 'moist': 856,\n",
              " 'wine': 857,\n",
              " 'heart': 858,\n",
              " 'sweetened': 859,\n",
              " 'ready': 860,\n",
              " 'locally': 861,\n",
              " 'processed': 862,\n",
              " 'burnt': 863,\n",
              " 'filter': 864,\n",
              " 'reasonable': 865,\n",
              " 'dressing': 866,\n",
              " 'buck': 867,\n",
              " 'creamer': 868,\n",
              " 'disgusting': 869,\n",
              " 'negative': 870,\n",
              " 'convenience': 871,\n",
              " 'happened': 872,\n",
              " 'shape': 873,\n",
              " 'despite': 874,\n",
              " 'tend': 875,\n",
              " 'interesting': 876,\n",
              " 'plenty': 877,\n",
              " 'biscuit': 878,\n",
              " 'yogurt': 879,\n",
              " 'raisin': 880,\n",
              " 'nature': 881,\n",
              " 'careful': 882,\n",
              " 'switched': 883,\n",
              " 'mom': 884,\n",
              " 'middle': 885,\n",
              " 'finding': 886,\n",
              " 'sip': 887,\n",
              " 'kettle': 888,\n",
              " 'included': 889,\n",
              " 'sea': 890,\n",
              " 'ask': 891,\n",
              " 'wow': 892,\n",
              " 'door': 893,\n",
              " 'comment': 894,\n",
              " 'safe': 895,\n",
              " 'balance': 896,\n",
              " 'sticky': 897,\n",
              " 'office': 898,\n",
              " 'grey': 899,\n",
              " 'subtle': 900,\n",
              " 'subscription': 901,\n",
              " 'drank': 902,\n",
              " 'bigger': 903,\n",
              " 'flat': 904,\n",
              " 'kit': 905,\n",
              " 'donut': 906,\n",
              " 'pour': 907,\n",
              " 'seriously': 908,\n",
              " 'term': 909,\n",
              " 'jelly': 910,\n",
              " 'previous': 911,\n",
              " 'consumer': 912,\n",
              " 'summer': 913,\n",
              " 'fry': 914,\n",
              " 'mixing': 915,\n",
              " 'american': 916,\n",
              " 'earth': 917,\n",
              " 'sit': 918,\n",
              " 'cent': 919,\n",
              " 'unpleasant': 920,\n",
              " 'greasy': 921,\n",
              " 'preservative': 922,\n",
              " 'eater': 923,\n",
              " 'traditional': 924,\n",
              " 'setting': 925,\n",
              " 'seal': 926,\n",
              " 'party': 927,\n",
              " 'costco': 928,\n",
              " 'satisfying': 929,\n",
              " 'melted': 930,\n",
              " 'shes': 931,\n",
              " 'vegan': 932,\n",
              " 'trouble': 933,\n",
              " 'bud': 934,\n",
              " 'trip': 935,\n",
              " 'spend': 936,\n",
              " 'clearly': 937,\n",
              " 'beat': 938,\n",
              " 'face': 939,\n",
              " 'selling': 940,\n",
              " 'friendly': 941,\n",
              " 'fructose': 942,\n",
              " 'craving': 943,\n",
              " 'pretzel': 944,\n",
              " 'liver': 945,\n",
              " 'tree': 946,\n",
              " 'personal': 947,\n",
              " 'eye': 948,\n",
              " 'tart': 949,\n",
              " 'training': 950,\n",
              " 'head': 951,\n",
              " 'mini': 952,\n",
              " 'lick': 953,\n",
              " 'particularly': 954,\n",
              " 'advertised': 955,\n",
              " 'perfectly': 956,\n",
              " 'pea': 957,\n",
              " 'garbage': 958,\n",
              " 'taco': 959,\n",
              " 'miss': 960,\n",
              " 'picked': 961,\n",
              " 'gourmet': 962,\n",
              " 'bother': 963,\n",
              " 'keep': 964,\n",
              " 'search': 965,\n",
              " 'floor': 966,\n",
              " 'odor': 967,\n",
              " 'running': 968,\n",
              " 'grind': 969,\n",
              " 'th': 970,\n",
              " 'zero': 971,\n",
              " 'root': 972,\n",
              " 'throwing': 973,\n",
              " 'pancake': 974,\n",
              " 'dessert': 975,\n",
              " 'apart': 976,\n",
              " 'mother': 977,\n",
              " 'literally': 978,\n",
              " 'suppose': 979,\n",
              " 'spot': 980,\n",
              " 'apparently': 981,\n",
              " 'earl': 982,\n",
              " 'crystal': 983,\n",
              " 'doubt': 984,\n",
              " 'fed': 985,\n",
              " 'filler': 986,\n",
              " 'vendor': 987,\n",
              " 'squeeze': 988,\n",
              " 'admit': 989,\n",
              " 'base': 990,\n",
              " 'youve': 991,\n",
              " 'brought': 992,\n",
              " 'flake': 993,\n",
              " 'lab': 994,\n",
              " 'wild': 995,\n",
              " 'spoon': 996,\n",
              " 'thai': 997,\n",
              " 'aware': 998,\n",
              " 'concerned': 999,\n",
              " 'kitty': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#show an example of what a tokenized and padded sentence looks like\n",
        "display(tweets_sentiment[0])\n",
        "\n",
        "#what the tokenizer object dictionary look like with each index and its\n",
        "#respective word where the first word is the most common and so on.\n",
        "tokenizer_sentiment.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2SyfKhYSdgp"
      },
      "source": [
        "**Label encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgh8zSOGaf3t",
        "outputId": "cf771769-8a0d-4c94-b881-441c725e84fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1040796, 1040796)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#show size of upsampled sarcasm data\n",
        "labels_sarcasm  = data_upsampled_sarcasm[['label']].values\n",
        "len(labels_sarcasm), len(tweets_sarcasm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKQZ3mKayMiK",
        "outputId": "5f7923ad-ca59-47ad-ed3d-d5cdf0a8e199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4 4 4 ... 0 0 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1832295"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Label encoding\n",
        "\n",
        "#this section turns the labels from being 1,2,3,4,5 for the sentiment analysis\n",
        "#to being hot encoded arrays so that they can be treated as the output data for training and testing\n",
        "labels_sentiment = np.array(data_upsampled_sentiment['label'])\n",
        "y = []\n",
        "for i in range(len(labels_sentiment)):\n",
        "    if labels_sentiment[i] == 1:\n",
        "        y.append(0)\n",
        "    if labels_sentiment[i] == 2:\n",
        "        y.append(1)\n",
        "    if labels_sentiment[i] == 3:\n",
        "        y.append(2)\n",
        "    if labels_sentiment[i] == 4:\n",
        "        y.append(3)\n",
        "    if labels_sentiment[i] == 5:\n",
        "        y.append(4)\n",
        "y = np.array(y)\n",
        "print(y)\n",
        "labels_sentiment = tf.keras.utils.to_categorical(y, 5, dtype=\"float32\")\n",
        "del y\n",
        "len(labels_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#this is what the hot encoded data labeling looks like\n",
        "#the star rating is selected with a 1 flag while all the other ones\n",
        "#are 0, so for example 3 stars would be [0,0,1,0,0]\n",
        "display(labels_sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0uyZ5n6Shlh"
      },
      "source": [
        "**Train_test_split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLoclUo8af3t",
        "outputId": "0377ae71-2ad3-4132-8486-bed40ceab7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "936716 104080 936716 104080\n",
            "1649065 183230 1649065 183230\n"
          ]
        }
      ],
      "source": [
        "#Splitting the data for training and testing purposes\n",
        "\n",
        "X_train_sarcasm, X_test_sarcasm, y_train_sarcasm, y_test_sarcasm = train_test_split(tweets_sarcasm,labels_sarcasm, test_size=0.1,random_state=1, shuffle = True)\n",
        "print (len(X_train_sarcasm),len(X_test_sarcasm),len(y_train_sarcasm),len(y_test_sarcasm))\n",
        "\n",
        "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(tweets_sentiment,labels_sentiment, test_size=0.1,random_state=1, shuffle = True)\n",
        "print (len(X_train_sentiment),len(X_test_sentiment),len(y_train_sentiment),len(y_test_sentiment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import the 200 dimensional pre-trained word2vec gloVe dataset\n",
        "EMBEDDING_FILE = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
        "\n",
        "def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "#this basically parses the gloVe word2vec embeddings file by going line by line\n",
        "#splitting up the words and numbers then turning it into a dictionary item which will\n",
        "#be later used to create the embedding matrices\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding = 'utf8'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('you',\n",
              " array([ 1.9640e-01,  6.7153e-01,  6.2976e-03,  2.5359e-01, -4.2097e-01,\n",
              "         3.8490e-01,  1.0378e+00, -1.8536e-01, -5.4244e-02, -1.0856e-01,\n",
              "         1.2146e-01,  4.7692e-02, -9.3228e-01, -2.7192e-01, -3.5060e-01,\n",
              "         1.1069e-01, -5.5099e-02, -7.9658e-02,  1.1767e-02,  1.4395e-01,\n",
              "        -2.5917e-02, -1.8253e-01,  2.5691e-02,  1.9619e-01,  1.0334e-01,\n",
              "         1.0731e+00,  4.1992e-01,  1.7083e-01,  9.1596e-01, -4.3398e-01,\n",
              "         1.6464e-01,  1.2715e-02,  9.5477e-02,  1.4490e-01, -2.9652e-01,\n",
              "        -1.0642e-01,  9.8389e-02,  1.7937e-01,  2.3289e-01, -2.2209e-01,\n",
              "         8.7770e-02, -1.8005e-01,  3.8678e-01, -3.2059e-02,  1.2020e-01,\n",
              "        -4.1741e-01,  2.6827e-01, -1.9769e-01,  2.1976e-02, -2.3585e-01,\n",
              "         1.4789e-01,  1.0173e-01, -1.0380e-01, -3.1954e-01,  6.3308e-01,\n",
              "        -5.1142e-02, -5.3209e-02,  2.9378e-01, -5.4395e-02,  4.3346e-05,\n",
              "         2.5585e-01, -1.8048e-01, -2.7030e-01,  7.7783e-03,  6.3585e-01,\n",
              "         1.4869e-01, -1.0854e-01, -2.4726e-01, -1.0869e-01, -2.4147e-01,\n",
              "         2.8059e-01,  1.3645e-01, -3.5610e-02, -6.8258e-01,  1.1957e-01,\n",
              "         1.1085e-02, -1.3771e-02, -1.7603e-01, -5.7826e-01, -4.1571e-02,\n",
              "        -1.2342e-01,  5.8091e-01, -4.4446e-02,  1.4833e-01, -8.1576e-02,\n",
              "        -1.5804e-01, -4.3868e-01, -9.0296e-02, -2.7891e-01, -3.2887e-02,\n",
              "         1.1688e-01,  5.4837e-02,  1.0082e-01,  2.4519e-01,  1.5062e-01,\n",
              "         3.8064e-02, -2.5851e-01, -2.3446e-01,  6.4198e-02,  1.2255e-01,\n",
              "        -5.2537e-01,  3.3350e-01, -2.3323e-01, -2.5780e-01,  2.9035e-01,\n",
              "         1.3483e-01,  6.5876e-02,  3.8322e-01,  1.1237e-01, -2.3038e-03,\n",
              "        -2.8184e-01,  1.7724e-01,  5.6759e-01,  2.4386e-01, -4.0227e-02,\n",
              "        -4.5024e-02, -3.4185e-01, -4.1455e-01, -4.1110e-02,  2.2438e-01,\n",
              "        -1.1626e-01,  2.0744e-01, -1.1859e-01, -7.7585e-02,  3.3058e-01,\n",
              "        -2.8058e-01,  4.9034e-02, -1.9482e-01,  6.7283e-02,  3.0070e-02,\n",
              "         6.9742e-02, -1.3861e-01,  6.2041e-01,  3.8005e-01, -5.0533e-02,\n",
              "         9.7378e-02,  2.7874e-01, -1.7388e-01,  3.7021e-01, -6.6593e-01,\n",
              "         1.4769e-01, -6.3142e-02, -1.2092e-01,  3.7639e-01, -2.0316e-01,\n",
              "         3.6495e-01,  2.5422e-01,  1.6774e-01,  5.0256e-02,  5.4627e-02,\n",
              "         1.0653e-01, -7.0859e-02, -6.7986e+00,  4.0561e-01, -3.8071e-01,\n",
              "         3.2075e-01, -3.3545e-01, -1.5599e-02, -4.6192e-01,  2.9860e-01,\n",
              "         5.1756e-01, -1.7136e-01, -8.3454e-01, -1.4496e-01, -3.6553e-01,\n",
              "        -1.8636e-01,  1.0103e-01,  3.6616e-01,  1.3110e-01,  5.1864e-01,\n",
              "        -4.1521e-01, -1.9650e-01, -6.8740e-03, -1.4722e-02, -3.3198e-01,\n",
              "         1.3165e-01,  2.9492e-01, -1.9796e-01, -2.8374e-01, -1.4532e-01,\n",
              "         1.9547e-01,  1.1138e-01,  1.2032e-01,  2.1849e-01,  4.7585e-01,\n",
              "        -7.8959e-01,  3.0750e-01,  1.0715e-02, -2.1146e-01, -2.0619e-02,\n",
              "        -1.1518e-01,  5.5363e-01, -1.8500e-01, -1.9653e-01,  1.0566e-01,\n",
              "        -2.6214e-01,  2.0397e-01, -3.0815e-01, -1.2312e-01,  4.8926e-02],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#this is what one item from the embedding dictionary looks like\n",
        "display(list(embeddings_index.items())[15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAhBzZnraf3u"
      },
      "outputs": [],
      "source": [
        "#turns the embeddings dictionary into a matrix\n",
        "all_embs = np.stack(list(embeddings_index.values()))\n",
        "\n",
        "#get the mean and standard deviation of the embedding values\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "#get how many dimensions we're using\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "# the ordered dictionary of words based on how often they show up in the sentiment data\n",
        "word_index = tokenizer_sentiment.word_index\n",
        "\n",
        "# how many usable words do we have\n",
        "nb_words = min(max_words, len(word_index))\n",
        "\n",
        "#### change below line if computing normal stats is too slow\n",
        "\n",
        "# create an embedding matrix of size nb_words x embed_size \n",
        "embedding_matrix_sentiment = embedding_matrix_sentiment = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "\n",
        "# fill in the embedding_matrix_sentiment matrix with gloVe data in the same order of the\n",
        "# sentiment tokenizer object so that it can be later accessed based on the tokenizer index\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_words: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_sentiment[i-1] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1ZWMQKaClAw"
      },
      "outputs": [],
      "source": [
        "#turns the embeddings dictionary into a matrix\n",
        "all_embs = np.stack(list(embeddings_index.values()))\n",
        "\n",
        "#get the mean and standard deviation of the embedding values\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "#get how many dimensions we're using\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "# the ordered dictionary of words based on how often they show up in the sarcasm datta\n",
        "word_index = tokenizer_sarcasm.word_index\n",
        "\n",
        "# how many usable words do we have\n",
        "nb_words = min(max_words, len(word_index))\n",
        "\n",
        "####change below line if computing normal stats is too slow\n",
        "\n",
        "# create an embedding matrix of size nb_words x embed_size \n",
        "embedding_matrix_sarcasm = embedding_matrix_sarcasm = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "\n",
        "# fill in the embedding_matrix_sentiment matrix with gloVe data in the same order of the\n",
        "# sarcasm tokenizer object so that it can be later accessed based on the tokenizer index\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_words: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_sarcasm[i-1] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gofyySMcSml5"
      },
      "source": [
        "**Machine Learning Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXmlvVrpaf3v"
      },
      "outputs": [],
      "source": [
        "# create an embedding layer for sentiment\n",
        "embedding_layer1 = layers.Embedding(nb_words, output_dim=200, input_length=max_len, weights = [embedding_matrix_sentiment], trainable = True)\n",
        "\n",
        "# create an embedding layer for sarcasm\n",
        "embedding_layer2 = layers.Embedding(nb_words, output_dim=200, input_length=max_len, weights = [embedding_matrix_sarcasm], trainable = True)\n",
        "\n",
        "# create an input layer for sentiment\n",
        "sequence_input1 = layers.Input(shape=(max_len,), name='sentiment_input')\n",
        "\n",
        "# create an input layer for sarcasm\n",
        "sequence_input2 = layers.Input(shape=(max_len,), name='sarcasm_input')\n",
        "\n",
        "# connect the input layer with embedding layer\n",
        "embedded_sequences1 = embedding_layer1(sequence_input1)\n",
        "embedded_sequences2 = embedding_layer2(sequence_input2)\n",
        "\n",
        "#sentiment branch\n",
        "nn1 = layers.Bidirectional(layers.LSTM(units=20 , dropout = 0.6))(embedded_sequences1)\n",
        "nn1 = layers.Dense(256, activation=\"relu\")(nn1)\n",
        "nn1 = layers.Dropout(rate=0.4)(nn1)\n",
        "nn1 = layers.Dense(32, activation=\"relu\")(nn1)\n",
        "nn1 = layers.Dropout(rate=0.4)(nn1)\n",
        "\n",
        "#sarcasm branch\n",
        "nn2 = layers.Bidirectional(layers.LSTM(units=20 ,dropout = 0.6))(embedded_sequences2)\n",
        "nn2 = layers.Dense(256, activation=\"relu\")(nn2)\n",
        "nn2 = layers.Dropout(rate=0.4)(nn2)\n",
        "nn2 = layers.Dense(32, activation=\"relu\")(nn2)\n",
        "nn2 = layers.Dropout(rate=0.4)(nn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCEchwsgaf3v"
      },
      "outputs": [],
      "source": [
        "# create the output layers\n",
        "sentiment_branch = layers.Dense(5, activation='softmax', name='sentiment_output')(nn1)\n",
        "sarcasm_branch = layers.Dense(1, activation='sigmoid', name='sarcasm_output')(nn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NHUxlaYaf3w",
        "outputId": "1735ee3a-6854-4840-e959-6e9897fe1ce4"
      },
      "outputs": [],
      "source": [
        "# instantiate the model and define the inputs and outputs\n",
        "model = Model(inputs = [sequence_input1, sequence_input2], outputs = [sentiment_branch, sarcasm_branch])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A4cBUwDaf3x",
        "outputId": "f5ddc1da-e2d3-4933-c16f-3a0ce475d217"
      },
      "outputs": [],
      "source": [
        "# pick RMSoptimizer as the loss function optimization method\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "# compile the model where the sentiment uses cateogorial crossentropy based loss function and sarcasm uses binary crossentropy based loss function\n",
        "model.compile(optimizer=opt, loss={'sentiment_output':'categorical_crossentropy','sarcasm_output':'binary_crossentropy'}, metrics=['acc'])\n",
        "checkpoint2 = [ModelCheckpoint(\"weights/with_embedding(LSTM)1.hdf5\", monitor='val_sentiment_output_acc', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-LJAu5Laf3x"
      },
      "outputs": [],
      "source": [
        "# a class that plots the loss function progress after every epoch\n",
        "\n",
        "class PlotProgress(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, entity='loss'):\n",
        "        self.entity = entity\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('{}'.format(self.entity)))\n",
        "        self.val_losses.append(logs.get('val_{}'.format(self.entity)))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        fig = plt.figure()\n",
        "        plt.plot(self.x, self.losses, label= 'training_loss', color = 'Black')\n",
        "        plt.plot(self.x, self.val_losses, label= 'validation_loss', ls = ':', color = 'Black')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        fig.patch.set_facecolor('white')\n",
        "        \n",
        "        #fig.patch.set_alpha(0.0)\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGB3K2vPaf3x"
      },
      "outputs": [],
      "source": [
        "# instantiate the plot progress class to be used while we train the model\n",
        "plot_progress = PlotProgress(entity='loss')\n",
        "\n",
        "# train the model based on the data we have defined earlier and plot the progress as we go\n",
        "# and detect an exception if we manually stop the cell from running\n",
        "try:\n",
        "    model.fit({'sentiment_input': X_train_sentiment,'sarcasm_input':X_train_sarcasm},\n",
        "              {'sentiment_output': y_train_sentiment, 'sarcasm_output': y_train_sarcasm},\n",
        "              epochs=10,\n",
        "              batch_size=128,\n",
        "              verbose=1,\n",
        "              callbacks=[plot_progress, checkpoint2],\n",
        "              validation_split=0.2,\n",
        "             )\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the newly trained model so we don't have to train it again when we want to use it\n",
        "model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Running Pepsi and Coke Tweets Through Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "tweet_dict = {\n",
        "    \"date\": [],\n",
        "    \"content\": [],\n",
        "    \"sentiment\": [],\n",
        "    \"sarcasm\": []\n",
        "}\n",
        "\n",
        "for file in os.listdir(\"coke tweets\"):\n",
        "    if file.endswith(\".png\") == False:\n",
        "        continue\n",
        "    imagepath = \"coke tweets/\" + file\n",
        "    image = Image.open(imagepath)\n",
        "\n",
        "    pixels = image.getdata()\n",
        "    pixels = list(pixels)\n",
        "    odating = [pixels[2+i*1200] for i in range(image.size[1])]\n",
        "    indices = [index for index, key in enumerate(odating) if key == (47,51,54,255)]\n",
        "\n",
        "    os.mkdir(\"coke tweets/\" + imagepath[12:17])\n",
        "    directory = \"coke tweets/\" + imagepath[12:17]\n",
        "\n",
        "    for i in range(int(len(indices)/2-1)):\n",
        "        imm = image.crop((0,indices[1+i*2],1200,indices[2+i*2]))\n",
        "        if imm.size[1] > 65+80:\n",
        "            imm.save(directory + \"/\" + str(i) + \".png\")\n",
        "\n",
        "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "\n",
        "\n",
        "    for file in files:\n",
        "        image = Image.open(os.path.join(directory, file))\n",
        "        newim = image.crop((120,65,1200,image.size[1]-80))\n",
        "        new_sentence = [pytesseract.image_to_string(newim)]\n",
        "        tokenized_sentence_sentiment = tokenizer_sentiment.texts_to_sequences(new_sentence)\n",
        "        tokenized_sentence_sarcasm = tokenizer_sarcasm.texts_to_sequences(new_sentence)\n",
        "        new_padded_sequence_sentiment = pad_sequences(tokenized_sentence_sentiment, maxlen=200)\n",
        "        new_padded_sequence_sarcasm = pad_sequences(tokenized_sentence_sarcasm, maxlen = 200)\n",
        "        predictions = model.predict({'sentiment_input': new_padded_sequence_sentiment,'sarcasm_input':new_padded_sequence_sarcasm})\n",
        "\n",
        "        tweet_dict[\"date\"].append(imagepath[12:17])\n",
        "        tweet_dict[\"content\"].append(new_sentence[0])\n",
        "        tweet_dict[\"sentiment\"].append(predictions[0][0].tolist())\n",
        "        tweet_dict[\"sarcasm\"].append(predictions[1][0][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the processed coke tweets data to an csv file for trading usage\n",
        "\n",
        "df = pd.DataFrame(tweet_dict)\n",
        "\n",
        "df.to_csv(\"coke tweets data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "tweet_dict = {\n",
        "    \"date\": [],\n",
        "    \"content\": [],\n",
        "    \"sentiment\": [],\n",
        "    \"sarcasm\": []\n",
        "}\n",
        "\n",
        "for file in os.listdir(\"pepsi tweets\"):\n",
        "    if file.endswith(\".png\") == False:\n",
        "        continue\n",
        "    imagepath = \"pepsi tweets/\" + file\n",
        "    image = Image.open(imagepath)\n",
        "\n",
        "    pixels = image.getdata()\n",
        "    pixels = list(pixels)\n",
        "    odating = [pixels[2+i*1200] for i in range(image.size[1])]\n",
        "    indices = [index for index, key in enumerate(odating) if key == (47,51,54,255)]\n",
        "\n",
        "    os.mkdir(\"pepsi tweets/\" + imagepath[13:18])\n",
        "    directory = \"pepsi tweets/\" + imagepath[13:18]\n",
        "\n",
        "    for i in range(int(len(indices)/2-1)):\n",
        "        imm = image.crop((0,indices[1+i*2],1200,indices[2+i*2]))\n",
        "        if imm.size[1] > 65+80:\n",
        "            imm.save(directory + \"/\" + str(i) + \".png\")\n",
        "\n",
        "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "\n",
        "\n",
        "    for file in files:\n",
        "        image = Image.open(os.path.join(directory, file))\n",
        "        newim = image.crop((120,65,1200,image.size[1]-80))\n",
        "        new_sentence = [pytesseract.image_to_string(newim)]\n",
        "        tokenized_sentence_sentiment = tokenizer_sentiment.texts_to_sequences(new_sentence)\n",
        "        tokenized_sentence_sarcasm = tokenizer_sarcasm.texts_to_sequences(new_sentence)\n",
        "        # display(tokenized_sentence_sentiment)\n",
        "        # display(tokenized_sentence_sarcasm)\n",
        "        new_padded_sequence_sentiment = pad_sequences(tokenized_sentence_sentiment, maxlen=200)\n",
        "        new_padded_sequence_sarcasm = pad_sequences(tokenized_sentence_sarcasm, maxlen = 200)\n",
        "        predictions = model.predict({'sentiment_input': new_padded_sequence_sentiment,'sarcasm_input':new_padded_sequence_sarcasm})\n",
        "\n",
        "        tweet_dict[\"date\"].append(imagepath[13:18])\n",
        "        tweet_dict[\"content\"].append(new_sentence[0])\n",
        "        tweet_dict[\"sentiment\"].append(predictions[0][0].tolist())\n",
        "        tweet_dict[\"sarcasm\"].append(predictions[1][0][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the processed pepsi tweets data to an csv file for trading usage\n",
        "\n",
        "df = pd.DataFrame(tweet_dict)\n",
        "\n",
        "df.to_csv(\"pepsi tweets data.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
